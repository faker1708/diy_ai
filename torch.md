
pytorch中默认浮点类型位32位

如何修改成8bit呢？？

a = a.half()
这样可以使用半精度


对不起，放弃了，找不到相关教程。
我也不知道我的显卡是否支持。

我讲实话，这个功能非常关键。

要不然，我们直接用char 自己来写个吧。
char 不就是8bit吗？？



现在有个损失越界的问题。
首先，我们是不能随意写函数的，因为需要torch来管理计算图，让它自动求导。

而有时权重 矩阵太离谱，会导致猜出来 的输出的损失过大，超过了float的范围。




https://github.com/xianhu/LearnPython/blob/master/python_visual_animation.py


现在可以做到 64层深，每层4个神经元，可以收敛，最初层改成128个神经元也可以收敛。
（batch_width = 2**10 he = 2**2)

但 7422 都很难 收敛 128 16 4 4 

128层 4宽，找不到。可能 需要非常大量的数据 
深度神经网络就需要高算力了。算得慢实在跑不动。


深度神经网络本质 是个串行的模型，这也是为什么深度学习比挖矿的并行性要差。

cpu 128层，可以


4  4 4 4 
这样的，gpu 轻松收敛，
cpu计算，也很快，但是收敛不了

同样的
深而浅的网络。
gpu 收敛不了。



越玩越乱了。不好玩。

